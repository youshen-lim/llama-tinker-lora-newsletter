# üìù Data Preparation and Annotation

## Overview

This document describes the data preparation process for fine-tuning LLaMA 3.2-1B with LoRA adapters. The process involves collecting newsletter content, manual annotation, and formatting into JSONL for training.

---

## Dataset Overview

### **Final Dataset Statistics**

| Split | Examples | Annotation Status | Purpose |
|-------|----------|-------------------|---------|
| **Training** | 101 | Fully annotated | Fine-tuning LoRA adapters |
| **Test** | 246 | Non-annotated | Evaluation with reference-based metrics |
| **Total Annotated** | 101 | Fully annotated | Complete annotated dataset |
| **Total Collected** | 347 | Mixed | All collected examples |

### **Data Split Rationale**

- **Training set**: 101 fully annotated examples (messages + metadata + annotation fields)
  - Sufficient for LoRA fine-tuning (50-200 examples recommended)
  - All records manually annotated with relevance scores, topics, companies, summaries

- **Test set**: 246 non-annotated examples (messages + metadata only, no annotation fields)
  - Large sample size provides statistical significance
  - Uses reference-based evaluation metrics (ROUGE, BERTScore, JSON validity, consistency)
  - Does not require annotation ground truth for evaluation
  - Efficient use of all collected data
- **No validation set**: Small dataset size makes separate validation unnecessary

---

## Data Format

### **JSONL Structure**

Each line in the JSONL file represents one training example with three main sections:

```json
{
  "messages": [...],
  "metadata": {...},
  "annotation": {...}
}
```

**Complete Example**:

```json
{
  "messages": [
    {
      "role": "user",
      "content": "Analyze: TLDR AI\n\n### OpenAI and Broadcom announce strategic collaboration\n**Reading Time:** 2 minute read\nüîó **Source:** https://openai.com/...\n\n**Summary:** OpenAI will design custom AI chips in collaboration with Broadcom starting in late 2026.\n\nJSON only:\n{\"relevance_score\": <1-10>, \"summary\": \"<1 sentence>\", \"insights\": [\"<key point 1>\", \"<key point 2>\"]}"
    },
    {
      "role": "assistant",
      "content": "{\n  \"relevance_score\": 9,\n  \"summary\": \"Major AI/tech breakthrough with significant industry impact\",\n  \"insights\": [\n    \"Revolutionary technological advancement\",\n    \"Industry transformation potential\"\n  ]\n}"
    }
  ],
  "metadata": {
    "source": "TLDR_digest",
    "content_length": 504,
    "generated": true,
    "created_at": "2025-10-16T18:09:08.842637",
    "extraction_method": "final_corrected_quality_filtered"
  },
  "annotation": {
    "relevance_score": 10,
    "topics": ["OpenAI", "AI Infrastructure"],
    "companies": ["OpenAI", "Broadcom"],
    "summary": "OpenAI and Broadcom announce strategic collaboration to deploy 10 gigawatts of AI-designed AI accelerator chips.",
    "quality": 3,
    "status": "corrected",
    "notes": ""
  }
}
```

### **Comprehensive Schema Definition**

#### **Top-Level Fields**

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `messages` | List[Dict] | ‚úÖ Yes | Conversation messages (user prompt + assistant response) |
| `metadata` | Dict | ‚úÖ Yes | Source and processing metadata |
| `annotation` | Dict | ‚úÖ Yes | Structured annotation with quality metrics |

#### **Messages Field**

Each message object contains:

| Field | Type | Required | Description | Example |
|-------|------|----------|-------------|---------|
| `role` | String | ‚úÖ Yes | Either "user" or "assistant" | "user" |
| `content` | String | ‚úÖ Yes | Message content | "Analyze: TLDR AI\n\n..." |

**User Message Content**:
- Prompt template: `"Analyze: {source_name}\n\n{newsletter_content}\n\nJSON only:\n{\"relevance_score\": <1-10>, \"summary\": \"<1 sentence>\", \"insights\": [\"<key point 1>\", \"<key point 2>\"]}"`
- Length: 500-3000 characters (truncated if longer)

**Assistant Message Content**:
- Format: JSON string with three fields:
  - `relevance_score`: Integer 1-10 (importance/relevance of content)
  - `summary`: String (1-2 sentence concise summary)
  - `insights`: Array of strings (2-5 key takeaways)

#### **Metadata Field**

Stores source and processing information:

| Field | Type | Required | Description | Example |
|-------|------|----------|-------------|---------|
| `source` | String | ‚úÖ Yes | Newsletter source identifier | "TLDR_digest" |
| `content_length` | Integer | ‚úÖ Yes | Character count of newsletter | 504 |
| `generated` | Boolean | ‚úÖ Yes | Whether response was generated by model | true |
| `created_at` | String (ISO 8601) | ‚úÖ Yes | Timestamp of creation | "2025-10-16T18:09:08.842637" |
| `extraction_method` | String | ‚úÖ Yes | Data extraction/processing method | "final_corrected_quality_filtered" |

#### **Annotation Field** ‚≠ê **AUTHORITATIVE SCHEMA**

Structured metadata for LoRA fine-tuning and evaluation:

**Required Fields**:

| Field | Type | Range | Description | Example |
|-------|------|-------|-------------|---------|
| `relevance_score` | Integer | 0-10 | Model's relevance assessment | 10 |
| `topics` | List[String] | 1+ items | Main topics covered | ["OpenAI", "AI Infrastructure"] |
| `companies` | List[String] | 0+ items | Companies mentioned | ["OpenAI", "Broadcom"] |
| `summary` | String | 1-500 chars | Concise content summary | "OpenAI and Broadcom announce..." |
| `quality` | Integer | **0-7** | Annotation quality score | 3 |
| `status` | String | See below | Annotation status | "corrected" |

**Quality Score [0-7] Interpretation**:
- **0**: Invalid/unusable annotation
- **1-2**: Poor quality (incomplete, inaccurate)
- **3-4**: Fair quality (acceptable, minor issues)
- **5-6**: Good quality (well-annotated, consistent)
- **7**: Excellent quality (perfect annotation)

**Status Values**:
- `"original"` - Initial annotation
- `"corrected"` - Manually corrected
- `"verified"` - Quality verified
- `"approved"` - Ready for production

**Optional Fields**:

| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `instruction` | String | Custom instruction for this example | "Focus on technical details" |
| `news_content` | String | Original news article text | "Full article content..." |
| `notes` | String | Annotator notes/comments | "High relevance due to..." |

### **Message Format Details**

**User Message**:
- Prompt template: `"Analyze: {source_name}\n\n{newsletter_content}\n\nJSON only:\n{\"relevance_score\": <1-10>, \"summary\": \"<1 sentence>\", \"insights\": [\"<key point 1>\", \"<key point 2>\"]}"`
- Content: Raw newsletter text (email body)
- Length: 500-3000 characters (truncated if longer)

**Assistant Message**:
- Format: JSON string
- Required fields:
  - `relevance_score`: Integer 1-10
  - `summary`: String (concise summary)
  - `insights`: Array of strings (key takeaways)

---

## Annotation Process

### **Step 1: Data Collection**

**Source**: Real newsletter emails from Gmail

**Collection Method**:
1. Export newsletters from Gmail
2. Extract email body content
3. Clean HTML formatting
4. Truncate to manageable length (500-3000 chars)

**Quality Criteria**:
- ‚úÖ Diverse topics (tech, business, research)
- ‚úÖ Varied newsletter sources
- ‚úÖ Different content structures
- ‚úÖ Mix of short and long newsletters

### **Step 2: Manual Annotation**

**Tool**: Custom Google Colab annotation widget

**Annotation Interface**:
```python
# Interactive widget with:
# - Newsletter content display
# - Relevance score slider (1-10)
# - Summary text area
# - Insights list (add/remove)
# - Save button
```

**Annotation Guidelines**:

1. **Relevance Score (1-10)**:
   - 1-3: Low relevance (spam, promotional)
   - 4-6: Medium relevance (general news)
   - 7-9: High relevance (actionable insights)
   - 10: Critical (urgent, high-impact)

2. **Summary**:
   - Length: 1-2 sentences
   - Focus: Main topic and key message
   - Style: Concise, informative
   - Example: "OpenAI announces GPT-4 Turbo with 3x cost reduction and 128K context window."

3. **Insights**:
   - Count: 2-5 insights per newsletter
   - Format: Bullet points
   - Content: Actionable takeaways, key facts, important details
   - Example: ["GPT-4 Turbo is 3x cheaper", "128K context window", "Improved instruction following"]

### **Step 3: Quality Control**

**Validation Checks**:
- ‚úÖ JSON format validity
- ‚úÖ Required fields present
- ‚úÖ Relevance score in range (1-10)
- ‚úÖ Summary not empty
- ‚úÖ At least 2 insights

**Consistency Checks**:
- ‚úÖ Similar newsletters have similar scores
- ‚úÖ Summaries are concise (not copy-paste)
- ‚úÖ Insights are distinct (not redundant)

### **Step 4: Data Splitting**

**Method**: Random split with stratification

```python
from sklearn.model_selection import train_test_split

# Split 80/20
train_data, test_data = train_test_split(
    annotated_data,
    test_size=0.2,
    random_state=42,
    stratify=relevance_scores  # Ensure balanced distribution
)
```

**Verification**:
- ‚úÖ Training set: 101 examples
- ‚úÖ Test set: 20 examples
- ‚úÖ No overlap between sets
- ‚úÖ Similar relevance score distribution

---

## Annotation Widget

### **Features**

1. **Newsletter Display**:
   - Shows full newsletter content
   - Syntax highlighting for readability
   - Scrollable for long content

2. **Annotation Controls**:
   - Relevance score slider (1-10)
   - Summary text area (auto-resize)
   - Insights list (add/remove dynamically)
   - Save button (validates and saves to JSONL)

3. **Progress Tracking**:
   - Shows current example number
   - Displays total examples
   - Progress bar

4. **Validation**:
   - Real-time JSON validation
   - Error messages for invalid input
   - Prevents saving incomplete annotations

### **Usage**

```python
# In Google Colab
from annotation_widget import AnnotationWidget

# Initialize widget
widget = AnnotationWidget(
    input_file='raw_newsletters.jsonl',
    output_file='annotated_newsletters.jsonl'
)

# Display widget
widget.display()

# Annotate each newsletter:
# 1. Read newsletter content
# 2. Set relevance score (1-10)
# 3. Write summary
# 4. Add insights
# 5. Click "Save"
# 6. Move to next newsletter
```

### **Code**

See `notebooks/JSONL_Annotation_Notebook_Final.ipynb` for complete implementation.

---

## Data Quality Metrics

### **Annotation Consistency**

| Metric | Value |
|--------|-------|
| **Inter-annotator agreement** | N/A (single annotator) |
| **Annotation time** | ~2-3 minutes per newsletter |
| **Total annotation time** | ~4-6 hours for 121 examples |

### **Content Diversity**

| Category | Count | Percentage |
|----------|-------|------------|
| **Technology** | 45 | 37% |
| **Business** | 32 | 26% |
| **Research** | 28 | 23% |
| **Other** | 16 | 13% |

### **Relevance Score Distribution**

| Score Range | Count | Percentage |
|-------------|-------|------------|
| **1-3 (Low)** | 12 | 10% |
| **4-6 (Medium)** | 48 | 40% |
| **7-9 (High)** | 55 | 45% |
| **10 (Critical)** | 6 | 5% |

---

## Data Augmentation

### **Techniques Considered**

1. **Paraphrasing**: Not used (risk of changing meaning)
2. **Back-translation**: Not used (small dataset, quality concerns)
3. **Synthetic generation**: Not used (prefer real newsletters)

### **Rationale**

- LoRA fine-tuning works well with 50-200 examples
- Real data preferred over synthetic for this use case
- Quality over quantity for semantic understanding tasks

---

## Data Storage

### **File Locations**

```
data/
‚îî‚îÄ‚îÄ processed/
    ‚îú‚îÄ‚îÄ newsletter_train_data.jsonl           # 101 training examples (simplified format)
    ‚îú‚îÄ‚îÄ newsletter_test_data.jsonl            # 246 test examples (non-annotated)
    ‚îî‚îÄ‚îÄ newsletter_training_annotated.jsonl   # 101 fully annotated examples (authoritative)
```

### **File Sizes and Formats**

| File | Records | Format | Annotation Status |
|------|---------|--------|-------------------|
| `newsletter_train_data.jsonl` | 101 | messages only | Simplified (training format) |
| `newsletter_test_data.jsonl` | 246 | messages + metadata | Non-annotated (reference-based eval) |
| `newsletter_training_annotated.jsonl` | 101 | messages + metadata + annotation | Fully annotated (authoritative) |

### **Backup**

- ‚úÖ Stored in Google Drive
- ‚úÖ Version controlled in Git
- ‚úÖ Backed up locally

---

## Lessons Learned

### **What Worked Well**

1. ‚úÖ **Custom annotation widget** - Much faster than manual JSON editing
2. ‚úÖ **Real newsletter data** - Better than synthetic examples
3. ‚úÖ **Structured JSON output** - Easy to validate and parse
4. ‚úÖ **Small dataset** - Sufficient for LoRA fine-tuning

### **Challenges**

1. ‚ö†Ô∏è **Time-consuming** - 4-6 hours for 101 annotated examples
2. ‚ö†Ô∏è **Subjectivity** - Relevance scores can be subjective
3. ‚ö†Ô∏è **HTML cleaning** - Some newsletters had complex formatting
4. ‚ö†Ô∏è **Scale limitation** - Only 101 examples annotated due to time constraints (246 non-annotated test examples used for evaluation)

### **Future Improvements**

1. **Semi-automated annotation** - Use base model to suggest annotations
2. **Multi-annotator** - Get multiple annotations for inter-rater reliability
3. **Active learning** - Prioritize uncertain examples for annotation
4. **Expand dataset** - Aim for 200-500 examples for better coverage

---

## Reproducibility

### **Annotation Workflow**

1. **Collect newsletters** from Gmail (export as MBOX or use Gmail API)
2. **Clean and format** newsletter content (remove HTML, truncate)
3. **Load annotation widget** in Google Colab
4. **Annotate each newsletter** (relevance score, summary, insights)
5. **Validate annotations** (JSON format, required fields)
6. **Split data** (80/20 train/test)
7. **Save to JSONL** files

### **Tools Required**

- Google Colab (for annotation widget)
- Python 3.8+
- Libraries: `ipywidgets`, `jsonschema`, `sklearn`

### **Time Estimate**

- Data collection: 1-2 hours (347 examples collected)
- Annotation: 4-6 hours (101 examples annotated)
- Validation and splitting: 30 minutes
- **Total**: 6-9 hours
- **Note**: 246 non-annotated examples used for test set (reference-based evaluation)

---

## References

- **JSONL Format**: [JSON Lines](https://jsonlines.org/)
- **LoRA Data Requirements**: Hu et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models.
- **Annotation Best Practices**: Pustejovsky & Stubbs (2012). Natural Language Annotation for Machine Learning.

