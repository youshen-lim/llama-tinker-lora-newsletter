# ============================================================================
# READY-TO-USE CODE CELLS FOR TINKER TRAINING WITH news_content DATA
# ============================================================================
#
# These cells fix the issue where Tinker was training on the "no_robots" 
# dataset instead of your news_content data.
#
# USAGE:
# 1. Copy Cell 1 and paste it BEFORE "Build sl_basic Training Config"
# 2. Replace the "Build sl_basic Training Config" cell with Cell 2
# 3. Run both cells
# 4. Then run "Run Tinker Training with nest_asyncio Fix"
#
# ============================================================================

# ============================================================================
# CELL 1: Create Custom Dataset Builder for news_content Data
# ============================================================================
# üìç INSERT THIS CELL BEFORE "Build sl_basic Training Config"

from tinker_cookbook.recipes import sl_basic
from pathlib import Path
import json

print("üîß Creating custom dataset builder for news_content data...")
print("="*80)

# Path to your training data
base_dir = "/content/drive/MyDrive/AI_Projects/news_content_FineTuning/training_data"
train_file_path = f"{base_dir}/news_content_train_data.jsonl"

# Verify file exists
if not Path(train_file_path).exists():
    print(f"‚ùå Training file not found: {train_file_path}")
    print("üí° Please run the train/test split code first!")
    print("\n‚ö†Ô∏è Cannot proceed without training data!")
else:
    print(f"‚úÖ Found training file: {train_file_path}")
    
    # Count examples and show sample
    with open(train_file_path, 'r') as f:
        lines = [line for line in f if line.strip()]
        num_examples = len(lines)
        
        # Show first example to verify it's news_content data
        if lines:
            first_example = json.loads(lines[0])
            print(f"\nüìä Training examples: {num_examples}")
            print(f"\nüìã First example preview:")
            if 'messages' in first_example:
                user_msg = first_example['messages'][0]['content'][:200]
                print(f"  User: {user_msg}...")
            else:
                print(f"  Keys: {list(first_example.keys())}")
    
    # Create dataset builder using FromConversationFileBuilder
    dataset_builder = sl_basic.FromConversationFileBuilder(
        conversation_file_path=train_file_path,
        common_config=sl_basic.ChatDatasetBuilderCommonConfig(
            model_name_for_tokenizer=MODEL_NAME_TINKER,
            renderer_name='llama3',  # Use llama3 renderer for Llama models
            max_length=32768,
            batch_size=4,  # Match your batch size
            train_on_what=sl_basic.TrainOnWhat.ALL_ASSISTANT_MESSAGES
        )
    )
    
    print(f"\n‚úÖ Custom dataset builder created!")
    print(f"  ‚Ä¢ File: {train_file_path}")
    print(f"  ‚Ä¢ Examples: {num_examples}")
    print(f"  ‚Ä¢ Model: {MODEL_NAME_TINKER}")
    print(f"  ‚Ä¢ Renderer: llama3")
    print(f"  ‚Ä¢ Batch size: 4")
    print(f"  ‚Ä¢ Training on: ALL_ASSISTANT_MESSAGES")
    
    print("\nüéØ Dataset builder ready!")
    print("   Next: Run the config creation cell to build training config")
    
print("="*80)


# ============================================================================
# CELL 2: Build sl_basic Training Config with Custom Dataset
# ============================================================================
# üìç REPLACE the existing "Build sl_basic Training Config" cell with this

from tinker_cookbook.recipes import sl_basic
from tinker_cookbook.hyperparam_utils import get_lr
import chz

print("‚öôÔ∏è Building sl_basic training configuration...")
print("="*80)

# Verify dataset_builder exists
if 'dataset_builder' not in globals():
    print("‚ùå ERROR: dataset_builder not found!")
    print("üí° Please run the 'Create Custom Dataset Builder' cell first!")
    print("="*80)
else:
    # Get recommended learning rate
    model_name = MODEL_NAME_TINKER
    recommended_lr = get_lr(model_name)
    
    print(f"\nüìã Training Parameters:")
    print(f"  ‚Ä¢ Model: {model_name}")
    print(f"  ‚Ä¢ Recommended LR: {recommended_lr:.6f}")
    print(f"  ‚Ä¢ LoRA rank: 32")
    print(f"  ‚Ä¢ Epochs: 1")
    print(f"  ‚Ä¢ Batch size: 4")
    print(f"  ‚Ä¢ Log path: /tmp/tinker-examples/sl_basic")
    
    # Create config blueprint
    blueprint = sl_basic.build_config_blueprint()
    
    # Apply custom settings INCLUDING the custom dataset builder
    config = blueprint.apply({
        'model_name': model_name,
        'learning_rate': recommended_lr,
        'lora_rank': 32,
        'num_epochs': 1,
        'save_every': 20,
        'log_path': '/tmp/tinker-examples/sl_basic',
        'dataset_builder': dataset_builder,  # ‚úÖ USE CUSTOM DATASET BUILDER
    }).make()
    
    print(f"\n‚úÖ Config created with CUSTOM dataset builder!")
    print(f"\nüìã Final Configuration:")
    print(f"  ‚Ä¢ Model: {config.model_name}")
    print(f"  ‚Ä¢ Learning rate: {config.learning_rate:.6f}")
    print(f"  ‚Ä¢ LoRA rank: {config.lora_rank}")
    print(f"  ‚Ä¢ Num epochs: {config.num_epochs}")
    print(f"  ‚Ä¢ Save every: {config.save_every} steps")
    print(f"  ‚Ä¢ Log path: {config.log_path}")
    print(f"  ‚Ä¢ Dataset builder: {type(config.dataset_builder).__name__}")
    
    # Verify it's using the custom builder
    if hasattr(config.dataset_builder, 'conversation_file_path'):
        print(f"\n‚úÖ VERIFIED: Using custom dataset from:")
        print(f"   {config.dataset_builder.conversation_file_path}")
    else:
        print(f"\n‚ö†Ô∏è WARNING: Dataset builder type: {type(config.dataset_builder).__name__}")
        print(f"   Expected: FromConversationFileBuilder")
    
    print("\nüéØ Config ready!")
    print("   Next: Run 'Run Tinker Training with nest_asyncio Fix' to start training")
    
    print("="*80)


# ============================================================================
# VERIFICATION CELL (Optional - Run after training starts)
# ============================================================================
# üìç Use this to verify training is using the correct dataset

import json

print("üîç Verifying training dataset...")
print("="*80)

# Check the config
if 'config' in globals():
    print(f"\nüìã Config Dataset Builder:")
    print(f"  ‚Ä¢ Type: {type(config.dataset_builder).__name__}")
    
    if hasattr(config.dataset_builder, 'conversation_file_path'):
        file_path = config.dataset_builder.conversation_file_path
        print(f"  ‚Ä¢ File: {file_path}")
        
        # Count examples in the file
        with open(file_path, 'r') as f:
            num_examples = sum(1 for line in f if line.strip())
        print(f"  ‚Ä¢ Examples: {num_examples}")
        
        # Show first example
        with open(file_path, 'r') as f:
            first_line = next(line for line in f if line.strip())
            first_example = json.loads(first_line)
            
        print(f"\nüìã First Training Example:")
        if 'messages' in first_example:
            for msg in first_example['messages']:
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')[:150]
                print(f"  {role}: {content}...")
        
        print(f"\n‚úÖ Training will use YOUR news_content data!")
        print(f"   NOT the 'no_robots' dataset")
    else:
        print(f"\n‚ö†Ô∏è WARNING: Using default dataset builder")
        print(f"   This will train on 'no_robots' dataset, not your news_content data!")
        print(f"   Please run the custom dataset builder cell first")
else:
    print("‚ùå Config not found. Please run the config creation cell first.")

print("="*80)


# ============================================================================
# TROUBLESHOOTING GUIDE
# ============================================================================

"""
üîß TROUBLESHOOTING:

1. **Error: "dataset_builder not found"**
   - Make sure you run CELL 1 before CELL 2
   - Check that CELL 1 completed successfully
   - Verify the training file exists at the path

2. **Training shows "no_robots" examples**
   - The config is using the default dataset builder
   - Re-run CELL 1 to create custom dataset_builder
   - Re-run CELL 2 to create config with custom builder
   - Then re-run training

3. **Error: "Training file not found"**
   - Run the train/test split code first
   - This creates news_content_train_data.jsonl
   - Verify the file exists in Google Drive

4. **Training examples don't match news_content data**
   - Check the file path in CELL 1
   - Verify news_content_train_data.jsonl contains your data
   - Run the verification cell to check what's being loaded

5. **Want to use different model or parameters**
   - Modify the parameters in CELL 2:
     - model_name: Change MODEL_NAME_TINKER
     - lora_rank: Default 32, can try 16 or 64
     - num_epochs: Default 1, can increase for more training
     - batch_size: Default 4, adjust based on GPU memory

üìö EXPECTED WORKFLOW:

1. ‚úÖ Run train/test split (creates news_content_train_data.jsonl)
2. ‚úÖ Run CELL 1 (creates custom dataset_builder)
3. ‚úÖ Run CELL 2 (creates config with custom dataset)
4. ‚úÖ Run training cell (trains on YOUR news_content data)
5. ‚úÖ Verify logged examples match your news_content data

üéØ SUCCESS INDICATORS:

- CELL 1 shows: "‚úÖ Custom dataset builder created!"
- CELL 2 shows: "‚úÖ VERIFIED: Using custom dataset from: ..."
- Training logs show YOUR news_content examples, not coffee/Texas/Calico stories
- Training completes with checkpoints saved

üí° REMEMBER:

The key fix is creating a FromConversationFileBuilder that points to your
news_content_train_data.jsonl file, instead of using the default NoRobotsBuilder
that loads the "no_robots" dataset.
"""

